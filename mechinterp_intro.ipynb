{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNL5PLkqQUygU6jotOf5jKd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cianadeveau/MechInterp/blob/main/mechinterp_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformer_lens"
      ],
      "metadata": {
        "id": "vdlYuLal7RWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhDhligo6xZQ"
      },
      "outputs": [],
      "source": [
        "import transformer_lens as tl\n",
        "from transformer_lens import HookedTransformer\n",
        "import torch\n",
        "import einops\n",
        "from jaxtyping import Float, Int\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
        "print(f\"Model has {model.cfg.n_layers} layers, {model.cfg.n_heads} heads per layer\")"
      ],
      "metadata": {
        "id": "nK-68PI67FY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture"
      ],
      "metadata": {
        "id": "ZYs-8mlEAO0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model configuration:\")\n",
        "print(f\"~ Vocabulary size: {model.cfg.d_vocab}\")\n",
        "print(f\"~ Embedding dimension: {model.cfg.d_model}\")\n",
        "print(f\"~ Number of layers: {model.cfg.n_layers}\")\n",
        "print(f\"~ Number of heads: {model.cfg.n_heads}\")\n",
        "print(f\"~ Attention Head dimension: {model.cfg.d_head}\")\n",
        "print(f\"~ MLP dimension: {model.cfg.d_mlp}\")"
      ],
      "metadata": {
        "id": "GCX4Ddjz9Gnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Weight matrix shapes:')\n",
        "for layer in range(model.cfg.n_layers):\n",
        "  print(f\"Layer {layer}:\")\n",
        "  print(f\" W_Q: {model.W_Q[layer].shape}\")\n",
        "  print(f\" W_K: {model.W_K[layer].shape}\")\n",
        "  print(f\" W_V: {model.W_V[layer].shape}\")\n",
        "  print(f\" W_O: {model.W_O[layer].shape}\")"
      ],
      "metadata": {
        "id": "Gn_uAAgz-Aqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QslTy6f3-IMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward Pass with Caching"
      ],
      "metadata": {
        "id": "yXrjekWRARO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The quick brown fox\"\n",
        "tokens = model.to_tokens(prompt, prepend_bos=False) #prepend eliminates the <|endoftext|> token\n",
        "logits = model(tokens)\n",
        "print(f\"Tokens shape: {tokens.shape}\")\n",
        "print(f\"Logits shape: {logits.shape}\")"
      ],
      "metadata": {
        "id": "qUnjvpWPAT5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Check tokens:')\n",
        "print(f'Tokens: {tokens}')\n",
        "print('Token Strings:')\n",
        "for i, token in enumerate(tokens[0]):\n",
        "  print(f'Position {i}: {repr(model.to_string(token))}')"
      ],
      "metadata": {
        "id": "7lG39TkxAgB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all activations with caching\n",
        "logits, cache = model.run_with_cache(prompt, prepend_bos=False)\n",
        "\n",
        "# Access specific activations\n",
        "print(\"Available activations:\")\n",
        "for key in list(cache.keys())[:20]:  # Show first 10\n",
        "    print(f\"  {key}: {cache[key].shape}\")\n",
        "\n",
        "# hook_attn_scores: raw attention scores before softmax\n",
        "# hook_pattern: attention weights after softmax - you use this most of the time because these weights determine information flow\n",
        "# hook_z: concatenated outputs of all attention heads before final output projection"
      ],
      "metadata": {
        "id": "otwirGL6BJ1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick reminder for how these transformations can be done with einops\n",
        "# attn_pattern: [batch, n_heads, seq_len, seq_len] = [1, 12, 4, 4]\n",
        "# v: [batch, seq_len, n_heads, d_head] = [1, 4, 12, 64]\n",
        "# need to do: pattern @ v for each head so that hinges on the n_heads axis\n",
        "# Rearrange v to match pattern's head dimension order\n",
        "# v_rearranged = einops.rearrange(cache['hook_v'], 'batch seq heads d_head -> batch heads seq d_head') # [1, 12, 4, 64]\n",
        "# Matrix multiplication (pattern @ v for each head)\n",
        "# z = torch.einsum('batch heads seq_out seq_in, batch heads seq_in d_head -> batch heads seq_out d_head',\n",
        "# cache['hook_pattern'], v_rearranged) # [1, 12, 4, 64]\n",
        "# Rearrange back to [batch, seq, heads, d_head]\n",
        "# z = einops.rearrange(z, 'batch heads seq d_head -> batch seq heads d_head')\n",
        "# Final z: [1, 4, 12, 64]\n",
        "\n",
        "# From z to attn_out\n",
        "# Concatenate all heads (flatten the head and d_head dimensions)\n",
        "# z_concat = einops.rearrange(z, 'batch seq n_heads d_head -> batch seq (n_heads d_head)')\n",
        "# z_concat: [1, 4, 768]\n",
        "\n",
        "# Apply output projection W_O\n",
        "# attn_out = z_concat @ W_O\n",
        "# attn_out: [1, 4, 768]"
      ],
      "metadata": {
        "id": "cCL3HMIgCwGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4gxnctNlhosJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention Pattern Visualization"
      ],
      "metadata": {
        "id": "I5A0YubhZ6aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a specific attention head's pattern\n",
        "layer = 0\n",
        "head = 7\n",
        "\n",
        "# get the attention pattern for this head\n",
        "attn_pattern = cache[f'blocks.{layer}.attn.hook_pattern'][0, head]\n",
        "print(f'Attention pattern shape: {attn_pattern.shape}')\n",
        "\n",
        "# show the tokens\n",
        "for i, token in enumerate(tokens[0]):\n",
        "  print(f'Position {i}: {repr(model.to_string(token))}')"
      ],
      "metadata": {
        "id": "w73U8D00Z9YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_pattern"
      ],
      "metadata": {
        "id": "KnGzczQqa36n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_labels = [model.to_string(token) for token in tokens[0]]\n",
        "\n",
        "fig = px.imshow(attn_pattern.detach().cpu().numpy(),\n",
        "                x=token_labels,\n",
        "                y=token_labels,\n",
        "                title=f'Attention Pattern - Layer {layer}, Head {head}',\n",
        "                labels=dict(x='Attended to', y='Attending From'))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "PfsD4xasbDla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common attention head types:\n",
        "# previous token heads: attend to the token right before\n",
        "# self-attention heads: attend mostly to themselves\n",
        "# broad attention heads: attend to \"The\" or other important tokens (like head 0)\n",
        "# positional heads: attend based on relative positions\n",
        "# induction heads: look for repeated patterns"
      ],
      "metadata": {
        "id": "QwVHSrX3ccDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "50_Qqak4h8rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Direct Logit Attribution"
      ],
      "metadata": {
        "id": "4gnht-89i3n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The captial of France is\"\n",
        "tokens = model.to_tokens(prompt, prepend_bos=False)\n",
        "logits, cache = model.run_with_cache(tokens)\n",
        "\n",
        "# answer token we want to analyze\n",
        "answer_token = model.to_single_token(\"Paris\")\n",
        "print(f'Looking for token: {repr(model.to_string(answer_token))}')"
      ],
      "metadata": {
        "id": "xadNENOni5VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decompose logits by layer\n",
        "per_layer_logits = torch.zeros(model.cfg.n_layers + 1, model.cfg.d_vocab)\n",
        "\n",
        "# Layer 0: Just embeddings (before any transformer layers)\n",
        "embed = cache[\"hook_embed\"]\n",
        "pos_embed = cache[\"hook_pos_embed\"]\n",
        "initial_residual = embed + pos_embed\n",
        "per_layer_logits[0] = initial_residual[0, -1] @ model.W_U # last token position\n",
        "\n",
        "# Each subsequent layer's contribution\n",
        "for layer in range(model.cfg.n_layers):\n",
        "  layer_output = cache[f'blocks.{layer}.hook_resid_post'][0, -1] # last token\n",
        "  per_layer_logits[layer + 1] = layer_output @ model.W_U\n",
        "\n",
        "# Analyze contributions to \"Paris\"\n",
        "contributions = per_layer_logits[:, answer_token]\n",
        "print(\"Logit contribution to 'Paris':\")\n",
        "print(f\"Embeddings: {contributions[0]:.3f}\")\n",
        "for layer in range(model.cfg.n_layers):\n",
        "  print(f\"After Layer {layer}: {contributions[layer+1]:.3f}\")"
      ],
      "metadata": {
        "id": "vA5Q7PWmkPie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_names = [\"Embeddings\"] + [f\"Layer {i}\" for i in range(model.cfg.n_layers)]\n",
        "\n",
        "fig = px.bar(\n",
        "    x=layer_names,\n",
        "    y=contributions.detach().cpu().numpy(),\n",
        "    labels={\"x\": \"Layer\", \"y\": \"Logit Contribution\"},\n",
        "    title=\"Logit Attribution to 'Paris' by Layer\"\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "b_OWEjqanHcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does the embedding predict by itself?\n",
        "final_token_embed = initial_residual[0, -1]  # Last token (\"is\")\n",
        "embedding_logits = final_token_embed @ model.W_U\n",
        "embedding_prediction = embedding_logits.argmax()\n",
        "\n",
        "print(f\"Embedding alone predicts: {repr(model.to_string(embedding_prediction))}\")\n",
        "print(f\"Embedding logit for 'Paris': {embedding_logits[answer_token]:.3f}\")\n",
        "\n",
        "# Let's also see what the top predictions are from embeddings alone\n",
        "top_5_embedding = torch.topk(embedding_logits, 5)\n",
        "print(\"\\nTop 5 predictions from embeddings alone:\")\n",
        "for i, (logit, token_id) in enumerate(zip(top_5_embedding.values, top_5_embedding.indices)):\n",
        "    print(f\"{i+1}. {repr(model.to_string(token_id))}: {logit:.3f}\")"
      ],
      "metadata": {
        "id": "2NK5jZPQo_ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare layer 0 vs layer 2 predictions\n",
        "layer_0_output = cache[\"blocks.0.hook_resid_post\"][0, -1]\n",
        "layer_2_output = cache[\"blocks.2.hook_resid_post\"][0, -1]\n",
        "\n",
        "layer_0_logits = layer_0_output @ model.W_U\n",
        "layer_2_logits = layer_2_output @ model.W_U\n",
        "\n",
        "print(f\"\\nAfter Layer 0 - Paris logit: {layer_0_logits[answer_token]:.3f}\")\n",
        "print(f\"After Layer 2 - Paris logit: {layer_2_logits[answer_token]:.3f}\")"
      ],
      "metadata": {
        "id": "lePdkrWgqBWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# representative layers\n",
        "token_labels = [model.to_string(token) for token in tokens[0]]\n",
        "layers_to_check = [0, 3, 6, 9, 11]  # Early to late\n",
        "head = 0  # We'll use head 0 for consistency\n",
        "\n",
        "fig, axes = plt.subplots(1, len(layers_to_check), figsize=(20, 4))\n",
        "\n",
        "for i, layer in enumerate(layers_to_check):\n",
        "    pattern = cache[f\"blocks.{layer}.attn.hook_pattern\"][0, head].detach().cpu().numpy()\n",
        "\n",
        "    im = axes[i].imshow(pattern, cmap='Blues')\n",
        "    axes[i].set_title(f'Layer {layer}, Head {head}')\n",
        "    axes[i].set_xticks(range(len(token_labels)))\n",
        "    axes[i].set_yticks(range(len(token_labels)))\n",
        "    axes[i].set_xticklabels(token_labels, rotation=45)\n",
        "    axes[i].set_yticklabels(token_labels)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pVIhqyGPqDso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look specifically at what the last token attends to across layers\n",
        "last_token_pos = -1\n",
        "\n",
        "print(\"What the last token attends to across layers:\")\n",
        "for layer in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]:\n",
        "    pattern = cache[f\"blocks.{layer}.attn.hook_pattern\"][0, 2]  # head 0\n",
        "    last_token_attention = pattern[last_token_pos, :]\n",
        "\n",
        "    print(f\"\\nLayer {layer}:\")\n",
        "    for pos, attention in enumerate(last_token_attention):\n",
        "        print(f\"  To '{token_labels[pos]}': {attention:.3f}\")"
      ],
      "metadata": {
        "id": "nm1vLsXjq6F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wtXlazHrr9zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See how much an attention head changes across layers"
      ],
      "metadata": {
        "id": "CDbmAOMIiwfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_pattern_distance(pattern1, pattern2):\n",
        "  '''how different two attention patterns are'''\n",
        "  flat1 = pattern1.flatten()\n",
        "  flat2 = pattern2.flatten()\n",
        "  return 1 - F.cosine_similarity(flat1, flat2, dim=0)\n",
        "\n",
        "change_scores = []\n",
        "for head in range(model.cfg.n_heads):\n",
        "  early_pattern = cache[f'blocks.1.attn.hook_pattern'][0, head]\n",
        "  late_pattern = cache[f'blocks.10.attn.hook_pattern'][0, head]\n",
        "  score = attention_pattern_distance(early_pattern, late_pattern)\n",
        "  change_scores.append(score.item())\n",
        "  print(f'Head {head}: Change score = {score: .3f}')\n",
        "\n",
        "most_changing = torch.tensor(change_scores).argmax().item()\n",
        "least_changing = torch.tensor(change_scores).argmin().item()\n",
        "\n",
        "print(f\"\\nMost changing head: {most_changing} (score: {change_scores[most_changing]:.3f})\")\n",
        "print(f\"Least changing head: {least_changing} (score: {change_scores[least_changing]:.3f}\")"
      ],
      "metadata": {
        "id": "_hh-wNaIiy2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize most changing head\n",
        "layers_to_show = [1, 4, 7, 10]\n",
        "\n",
        "fig, axes = plt.subplots(1, len(layers_to_show), figsize=(16,4))\n",
        "\n",
        "for i, layer in enumerate(layers_to_show):\n",
        "  pattern = cache[f\"blocks.{layer}.attn.hook_pattern\"][0, most_changing].detach().cpu().numpy()\n",
        "\n",
        "  im = axes[i].imshow(pattern, cmap='Blues', vmin=0, vmax=1)\n",
        "  axes[i].set_title(f'Head {most_changing}, Layer {layer}')\n",
        "  axes[i].set_xticks(range(len(token_labels)))\n",
        "  axes[i].set_yticks(range(len(token_labels)))\n",
        "  axes[i].set_xticklabels(token_labels, rotation=45, fontsize=8)\n",
        "  axes[i].set_yticklabels(token_labels, fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LfDg03yHi794"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the most stable head\n",
        "stable_head = least_changing\n",
        "layers_to_show = [1, 4, 7, 10]\n",
        "\n",
        "fig, axes = plt.subplots(1, len(layers_to_show), figsize=(16, 4))\n",
        "\n",
        "for i, layer in enumerate(layers_to_show):\n",
        "    pattern = cache[f\"blocks.{layer}.attn.hook_pattern\"][0, stable_head].detach().cpu().numpy()\n",
        "\n",
        "    im = axes[i].imshow(pattern, cmap='Blues', vmin=0, vmax=1)\n",
        "    axes[i].set_title(f'Head {stable_head}, Layer {layer}')\n",
        "    axes[i].set_xticks(range(len(token_labels)))\n",
        "    axes[i].set_yticks(range(len(token_labels)))\n",
        "    axes[i].set_xticklabels(token_labels, rotation=45, fontsize=8)\n",
        "    axes[i].set_yticklabels(token_labels, fontsize=8)\n",
        "\n",
        "plt.suptitle(f'Evolution of Least Changing Head ({stable_head})')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a-TeGJr3_glD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking to see if the first token becomes a repository for geographical/captial information\n",
        "# Track how \"The\" token's representation evolves\n",
        "the_position = 1  # \"The\" is at position 1\n",
        "\n",
        "print(\"Evolution of 'The' token's representation:\")\n",
        "for layer in [0, 3, 6, 9, 11]:\n",
        "    if layer == 0:\n",
        "        # Before any processing\n",
        "        resid = cache[\"hook_embed\"][0, the_position] + cache[\"hook_pos_embed\"][0, the_position]\n",
        "    else:\n",
        "        # After layer processing\n",
        "        resid = cache[f\"blocks.{layer-1}.hook_resid_post\"][0, the_position]\n",
        "\n",
        "    # Project to vocabulary to see what \"The\" is \"thinking about\"\n",
        "    logits = resid @ model.W_U\n",
        "    top_tokens = torch.topk(logits, 5)\n",
        "\n",
        "    print(f\"\\nAfter layer {layer-1 if layer > 0 else 'embeddings'}:\")\n",
        "    print(\"Top concepts in 'The' representation:\")\n",
        "    for token_id, score in zip(top_tokens.indices, top_tokens.values):\n",
        "        print(f\"  {repr(model.to_string(token_id))}: {score:.2f}\")"
      ],
      "metadata": {
        "id": "F7BZAuXQkiwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of vocabulary projection, let's see what information flows TO \"The\"\n",
        "the_position = 1\n",
        "\n",
        "print(\"What information flows TO 'The' token across layers:\")\n",
        "for layer in [1, 4, 7, 10]:\n",
        "    # Look at how much each position attends TO \"The\" (column 1)\n",
        "    patterns = cache[f\"blocks.{layer}.attn.hook_pattern\"][0]  # All heads\n",
        "\n",
        "    # Average across all heads to see overall attention TO \"The\"\n",
        "    avg_attention_to_the = patterns[:, :, the_position].mean(dim=0)\n",
        "\n",
        "    print(f\"\\nLayer {layer} - Average attention TO 'The':\")\n",
        "    for pos, attn in enumerate(avg_attention_to_the):\n",
        "        print(f\"  From '{token_labels[pos]}': {attn:.3f}\")"
      ],
      "metadata": {
        "id": "71Q8lcuul3KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1MBYAjbeBRXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Patching"
      ],
      "metadata": {
        "id": "l0uZwNzVE1YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_prompt = 'The capital of France is'\n",
        "corrupted_prompt = 'The capital of Spain is'\n",
        "\n",
        "# Get activations\n",
        "clean_tokens = model.to_tokens(clean_prompt, prepend_bos=False)\n",
        "corrupted_tokens = model.to_tokens(corrupted_prompt, prepend_bos=False)\n",
        "\n",
        "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
        "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
        "\n",
        "# Predictions\n",
        "print(\"Clean prediction:\", model.to_string(clean_logits[0, -1].argmax()))\n",
        "print(\"Corrupted prediction:\", model.to_string(corrupted_logits[0, -1].argmax()))\n",
        "\n",
        "# Target tokens\n",
        "paris_token = model.to_single_token(\" Paris\")\n",
        "madrid_token = model.to_single_token(\" Madrid\")\n",
        "\n",
        "print(f\"\\nClean logit for Paris: {clean_logits[0, -1, paris_token]:.3f}\")\n",
        "print(f\"Corrupted logit for Paris: {corrupted_logits[0, -1, paris_token]:.3f}\")"
      ],
      "metadata": {
        "id": "kfZzjNVNE3Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace activations from the corrupted run with activations from the clean run to see if it can fix the model behavior\n",
        "def patch_residual_stream(corrupted_cache, clean_cache, layer, position=-1):\n",
        "    \"\"\"Patch the residual stream at a specific layer and position\"\"\"\n",
        "    def patch_hook(activation, hook):\n",
        "        # Replace the corrupted activation with the clean one\n",
        "        activation[:, position, :] = clean_cache[hook.name][:, position, :]\n",
        "        return activation\n",
        "\n",
        "    # Run the corrupted input but with patched activation\n",
        "    patched_logits = model.run_with_hooks(\n",
        "        corrupted_tokens,\n",
        "        fwd_hooks=[(f\"blocks.{layer}.hook_resid_post\", patch_hook)]\n",
        "    )\n",
        "\n",
        "    return patched_logits\n",
        "\n",
        "# Test patching at different layers\n",
        "print(\"Patching residual stream at different layers:\")\n",
        "print(f\"Original corrupted logit for Paris: {corrupted_logits[0, -1, paris_token]:.3f}\")\n",
        "print(f\"Clean logit for Paris: {clean_logits[0, -1, paris_token]:.3f}\")\n",
        "\n",
        "for layer in [3, 6, 9, 11]:\n",
        "    patched_logits = patch_residual_stream(corrupted_cache, clean_cache, layer)\n",
        "    patched_paris_logit = patched_logits[0, -1, paris_token]\n",
        "\n",
        "    print(f\"Layer {layer} patched - Paris logit: {patched_paris_logit:.3f}\")\n",
        "    print(f\"  Prediction: {model.to_string(patched_logits[0, -1].argmax())}\")"
      ],
      "metadata": {
        "id": "oidSXAKhHWKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 5 predictions for each condition:\")\n",
        "\n",
        "# Clean run\n",
        "clean_top5 = torch.topk(clean_logits[0, -1], 5)\n",
        "print(\"\\nClean predictions:\")\n",
        "for i, (logit, token_id) in enumerate(zip(clean_top5.values, clean_top5.indices)):\n",
        "    print(f\"{i+1}. {repr(model.to_string(token_id))}: {logit:.3f}\")\n",
        "\n",
        "# Corrupted run\n",
        "corrupted_top5 = torch.topk(corrupted_logits[0, -1], 5)\n",
        "print(\"\\nCorrupted predictions:\")\n",
        "for i, (logit, token_id) in enumerate(zip(corrupted_top5.values, corrupted_top5.indices)):\n",
        "    print(f\"{i+1}. {repr(model.to_string(token_id))}: {logit:.3f}\")\n",
        "\n",
        "# Patched run\n",
        "patched_logits = patch_residual_stream(corrupted_cache, clean_cache, 9) # can update the layer to see how it changes over time\n",
        "patched_top5 = torch.topk(patched_logits[0, -1], 5)\n",
        "print(\"\\nPatched predictions:\")\n",
        "for i, (logit, token_id) in enumerate(zip(patched_top5.values, patched_top5.indices)):\n",
        "    print(f\"{i+1}. {repr(model.to_string(token_id))}: {logit:.3f}\")\n",
        "\n",
        "# And check where \"Paris\" ranks in each\n",
        "paris_rank_clean = (clean_logits[0, -1].argsort(descending=True) == paris_token).nonzero().item()\n",
        "paris_rank_corrupted = (corrupted_logits[0, -1].argsort(descending=True) == paris_token).nonzero().item()\n",
        "paris_rank_patched = (patched_logits[0, -1].argsort(descending=True) == paris_token).nonzero().item()\n",
        "print(f\"\\nParis ranking - Clean: {paris_rank_clean+1}, Corrupted: {paris_rank_corrupted+1}, Patched: {paris_rank_patched+1}\")"
      ],
      "metadata": {
        "id": "q-82T87qIaUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_patching_effect(clean_cache, corrupted_cache, layer):\n",
        "    \"\"\"Measure how much patching affects Paris ranking/logit\"\"\"\n",
        "\n",
        "    # Patch at this layer\n",
        "    patched_logits = patch_residual_stream(corrupted_cache, clean_cache, layer)\n",
        "\n",
        "    # Get Paris logit and ranking\n",
        "    paris_logit = patched_logits[0, -1, paris_token]\n",
        "    paris_rank = (patched_logits[0, -1].argsort(descending=True) == paris_token).nonzero().item() + 1\n",
        "\n",
        "    return paris_logit, paris_rank\n",
        "\n",
        "# Test across multiple layers\n",
        "print(\"Patching effect across layers:\")\n",
        "print(f\"Baseline (corrupted) - Paris rank: {paris_rank_corrupted+1}\")\n",
        "print(f\"Target (clean) - Paris rank: {paris_rank_clean+1}\")\n",
        "\n",
        "for layer in range(0, 12, 1):  # Every other layer\n",
        "    paris_logit, paris_rank = measure_patching_effect(clean_cache, corrupted_cache, layer)\n",
        "    print(f\"Layer {layer} patched - Paris rank: {paris_rank}, logit: {paris_logit:.3f}\")"
      ],
      "metadata": {
        "id": "zWAEQxbZJuyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10XB8m00NF_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path Patching"
      ],
      "metadata": {
        "id": "_O_a8T7XQx1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First testing the individal heads in Layer 8 which has the largest jump up in logit value"
      ],
      "metadata": {
        "id": "nCaCJlmCSchR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def patch_attention_head(layer, head, clean_cache, corrupted_cache):\n",
        "    \"\"\"Patch a specific attention head's output\"\"\"\n",
        "    def head_patch_hook(activation, hook):\n",
        "        # activation shape: [batch, seq_len, n_heads, d_head]\n",
        "        # Replace just this head's output with the clean version\n",
        "        activation[:, :, head, :] = clean_cache[hook.name][:, :, head, :]\n",
        "        return activation\n",
        "\n",
        "    # Patch at hook_z (before concatenation and W_O)\n",
        "    patched_logits = model.run_with_hooks(\n",
        "        corrupted_tokens,\n",
        "        fwd_hooks=[(f\"blocks.{layer}.attn.hook_z\", head_patch_hook)]\n",
        "    )\n",
        "\n",
        "    # Get Paris ranking\n",
        "    paris_rank = (patched_logits[0, -1].argsort(descending=True) == paris_token).nonzero().item() + 1\n",
        "    return paris_rank\n",
        "\n",
        "# Test each head in Layer 8\n",
        "print(\"Testing individual attention heads in Layer 8:\")\n",
        "print(f\"Baseline (corrupted): Paris rank ~{paris_rank_corrupted+1}\")\n",
        "print(f\"Full Layer 8 patch: Paris rank ~13\")\n",
        "\n",
        "for head in range(model.cfg.n_heads):\n",
        "    paris_rank = patch_attention_head(8, head, clean_cache, corrupted_cache)\n",
        "    print(f\"Head {head}: Paris rank {paris_rank}\")"
      ],
      "metadata": {
        "id": "w-KTAjzUQ17y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at Head 8.11's attention pattern for both clean and corrupted\n",
        "layer, head = 8, 11\n",
        "\n",
        "print(\"Head 8.11 attention patterns:\")\n",
        "print(\"\\nClean (France) attention:\")\n",
        "clean_pattern = clean_cache[f\"blocks.{layer}.attn.hook_pattern\"][0, head]\n",
        "for i, token in enumerate(clean_tokens[0]):\n",
        "    token_str = model.to_string(token)\n",
        "    attention_from_last = clean_pattern[-1, i]  # What does last token attend to?\n",
        "    print(f\"  '{token_str}': {attention_from_last:.3f}\")\n",
        "\n",
        "print(\"\\nCorrupted (Spain) attention:\")\n",
        "corrupted_pattern = corrupted_cache[f\"blocks.{layer}.attn.hook_pattern\"][0, head]\n",
        "for i, token in enumerate(corrupted_tokens[0]):\n",
        "    token_str = model.to_string(token)\n",
        "    attention_from_last = corrupted_pattern[-1, i]\n",
        "    print(f\"  '{token_str}': {attention_from_last:.3f}\")"
      ],
      "metadata": {
        "id": "gtzgwI56Su6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test if Head 8.11 is specifically a \"capital city\" head\n",
        "# Let's see what it does with different country contexts\n",
        "\n",
        "test_prompts = [\n",
        "    \"The capital of Germany is\",\n",
        "    \"The capital of Italy is\",\n",
        "    \"The capital of Japan is\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    tokens = model.to_tokens(prompt, prepend_bos=False)\n",
        "    _, cache = model.run_with_cache(tokens)\n",
        "\n",
        "    pattern = cache[f\"blocks.8.attn.hook_pattern\"][0, 11]  # Head 8.11\n",
        "    print(f\"\\n'{prompt}':\")\n",
        "    for i, token in enumerate(tokens[0]):\n",
        "        token_str = model.to_string(token)\n",
        "        attention_from_last = pattern[-1, i]\n",
        "        print(f\"  '{token_str}': {attention_from_last:.3f}\")"
      ],
      "metadata": {
        "id": "dbTBB34FTNDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def patch_mlp_output(layer, clean_cache, corrupted_cache):\n",
        "    \"\"\"Patch MLP output for a specific layer\"\"\"\n",
        "    def mlp_patch_hook(activation, hook):\n",
        "        activation[:, :, :] = clean_cache[hook.name][:, :, :]\n",
        "        return activation\n",
        "\n",
        "    patched_logits = model.run_with_hooks(\n",
        "        corrupted_tokens,\n",
        "        fwd_hooks=[(f\"blocks.{layer}.hook_mlp_out\", mlp_patch_hook)]\n",
        "    )\n",
        "\n",
        "    # Get Paris ranking\n",
        "    paris_rank = (patched_logits[0, -1].argsort(descending=True) == paris_token).nonzero().item() + 1\n",
        "    return paris_rank\n",
        "\n",
        "# Test MLP layers in our critical range\n",
        "print(\"Testing MLP layers:\")\n",
        "print(f\"Baseline (corrupted): Paris rank ~{paris_rank_corrupted+1}\")\n",
        "\n",
        "for layer in [7, 8, 9, 10, 11]:\n",
        "    paris_rank = patch_mlp_output(layer, clean_cache, corrupted_cache)\n",
        "    print(f\"MLP Layer {layer}: Paris rank {paris_rank}\")"
      ],
      "metadata": {
        "id": "JsLXiT9RUqVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def patch_both_head_and_mlp(layer, head, clean_cache, corrupted_cache):\n",
        "    \"\"\"Patch both attention head and MLP for the same layer\"\"\"\n",
        "    def head_patch_hook(activation, hook):\n",
        "        activation[:, :, head, :] = clean_cache[hook.name][:, :, head, :]\n",
        "        return activation\n",
        "\n",
        "    def mlp_patch_hook(activation, hook):\n",
        "        activation[:, :, :] = clean_cache[hook.name][:, :, :]\n",
        "        return activation\n",
        "\n",
        "    patched_logits = model.run_with_hooks(\n",
        "        corrupted_tokens,\n",
        "        fwd_hooks=[\n",
        "            (f\"blocks.{layer}.attn.hook_z\", head_patch_hook),\n",
        "            (f\"blocks.{layer}.hook_mlp_out\", mlp_patch_hook)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    paris_rank = (patched_logits[0, -1].argsort(descending=True) == paris_token).nonzero().item() + 1\n",
        "    return paris_rank\n",
        "\n",
        "# Test Layer 8 with both components\n",
        "combined_rank = patch_both_head_and_mlp(8, 11, clean_cache, corrupted_cache)\n",
        "print(f\"Layer 8 - Head 8.11 + MLP together: Paris rank {combined_rank}\")\n",
        "# if they were doing the same job combining wouldn't help much but since this improves performance then they are doing complementary actions"
      ],
      "metadata": {
        "id": "0NuIOjDzU4WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test if Layer 9 or 10 also have attention+MLP cooperation\n",
        "for test_layer in [9, 10]:\n",
        "    print(f\"\\nTesting Layer {test_layer} combinations:\")\n",
        "\n",
        "    # Test MLP alone\n",
        "    mlp_rank = patch_mlp_output(test_layer, clean_cache, corrupted_cache)\n",
        "    print(f\"MLP Layer {test_layer}: Paris rank {mlp_rank}\")\n",
        "\n",
        "    # Test a few promising attention heads + MLP\n",
        "    for head in [0, 6, 11]:  # Try a few different heads\n",
        "        combined_rank = patch_both_head_and_mlp(test_layer, head, clean_cache, corrupted_cache)\n",
        "        if combined_rank < 50:  # Only print promising ones\n",
        "            print(f\"  Head {head} + MLP: Paris rank {combined_rank}\")"
      ],
      "metadata": {
        "id": "bgkpMAcMWg22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try patching multiple layers and heads then to test for a Path\n",
        "def patch_multi_layer_circuit(clean_cache, corrupted_cache):\n",
        "    \"\"\"Patch the full factual recall circuit across layers\"\"\"\n",
        "    def head_8_patch(activation, hook):\n",
        "        activation[:, :, 11, :] = clean_cache[hook.name][:, :, 11, :]\n",
        "        return activation\n",
        "\n",
        "    def mlp_8_patch(activation, hook):\n",
        "        activation[:, :, :] = clean_cache[hook.name][:, :, :]\n",
        "        return activation\n",
        "\n",
        "    def head_10_patch(activation, hook):\n",
        "        activation[:, :, 0, :] = clean_cache[hook.name][:, :, 0, :]\n",
        "        return activation\n",
        "\n",
        "    def mlp_10_patch(activation, hook):\n",
        "        activation[:, :, :] = clean_cache[hook.name][:, :, :]\n",
        "        return activation\n",
        "\n",
        "    patched_logits = model.run_with_hooks(\n",
        "        corrupted_tokens,\n",
        "        fwd_hooks=[\n",
        "            (\"blocks.8.attn.hook_z\", head_8_patch),\n",
        "            (\"blocks.8.hook_mlp_out\", mlp_8_patch),\n",
        "            (\"blocks.10.attn.hook_z\", head_10_patch),\n",
        "            (\"blocks.10.hook_mlp_out\", mlp_10_patch)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    paris_rank = (patched_logits[0, -1].argsort(descending=True) == paris_token).nonzero().item() + 1\n",
        "    return paris_rank\n",
        "\n",
        "# Test the full circuit\n",
        "full_circuit_rank = patch_multi_layer_circuit(clean_cache, corrupted_cache)\n",
        "print(f\"Full circuit (Layer 8: Head 11 + MLP, Layer 10: Head 0 + MLP): Paris rank {full_circuit_rank}\")"
      ],
      "metadata": {
        "id": "qQWLzs4kXDKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# factual recall circuit above! not stored in one place - distributed computation, different layers have different roles"
      ],
      "metadata": {
        "id": "eOZWN22jXdXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing different types of facts"
      ],
      "metadata": {
        "id": "XW9gXE2_X05s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def patch_multi_layer_circuit_flexible(clean_cache, corrupted_cache, target_token_id):\n",
        "    \"\"\"Patch the circuit but handle different sequence lengths\"\"\"\n",
        "    def head_8_patch(activation, hook):\n",
        "        # Only patch the last position (where prediction happens)\n",
        "        activation[:, -1, 11, :] = clean_cache[hook.name][:, -1, 11, :]\n",
        "        return activation\n",
        "\n",
        "    def mlp_8_patch(activation, hook):\n",
        "        activation[:, -1, :] = clean_cache[hook.name][:, -1, :]\n",
        "        return activation\n",
        "\n",
        "    def head_10_patch(activation, hook):\n",
        "        activation[:, -1, 0, :] = clean_cache[hook.name][:, -1, 0, :]\n",
        "        return activation\n",
        "\n",
        "    def mlp_10_patch(activation, hook):\n",
        "        activation[:, -1, :] = clean_cache[hook.name][:, -1, :]\n",
        "        return activation\n",
        "\n",
        "    # Get the corrupted tokens for this specific test\n",
        "    patched_logits = model.run_with_hooks(\n",
        "        corrupted_tokens,  # Use the current corrupted tokens\n",
        "        fwd_hooks=[\n",
        "            (\"blocks.8.attn.hook_z\", head_8_patch),\n",
        "            (\"blocks.8.hook_mlp_out\", mlp_8_patch),\n",
        "            (\"blocks.10.attn.hook_z\", head_10_patch),\n",
        "            (\"blocks.10.hook_mlp_out\", mlp_10_patch)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Get ranking for the target token\n",
        "    target_rank = (patched_logits[0, -1].argsort(descending=True) == target_token_id).nonzero().item() + 1\n",
        "    return target_rank"
      ],
      "metadata": {
        "id": "Y2UGaqmmYzxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test different types of facts\n",
        "fact_tests = [\n",
        "    # Geographical (controls)\n",
        "    (\"The capital of Germany is\", \"The capital of Italy is\", \" Berlin\"),\n",
        "\n",
        "    # Person-profession relationships\n",
        "    (\"The author of Harry Potter is\", \"The author of Lord of the Rings is\", \" Rowling\"),\n",
        "\n",
        "    # Company-founder relationships\n",
        "    (\"The founder of Microsoft is\", \"The founder of Apple is\", \" Gates\"),\n",
        "\n",
        "    # Sports relationships\n",
        "    (\"The winner of the 2020 Olympics marathon was\", \"The winner of the 2016 Olympics marathon was\", \" Kipchoge\"),\n",
        "\n",
        "    # Scientific relationships\n",
        "    (\"The discoverer of penicillin was\", \"The discoverer of DNA structure was\", \" Fleming\")\n",
        "]\n",
        "\n",
        "def patch_multi_layer_circuit_flexible(clean_cache, corrupted_cache, corrupted_tokens):\n",
        "    \"\"\"Patch the circuit and return the patched logits\"\"\"\n",
        "    def head_8_patch(activation, hook):\n",
        "        activation[:, -1, 11, :] = clean_cache[hook.name][:, -1, 11, :]\n",
        "        return activation\n",
        "\n",
        "    def mlp_8_patch(activation, hook):\n",
        "        activation[:, -1, :] = clean_cache[hook.name][:, -1, :]\n",
        "        return activation\n",
        "\n",
        "    def head_10_patch(activation, hook):\n",
        "        activation[:, -1, 0, :] = clean_cache[hook.name][:, -1, 0, :]\n",
        "        return activation\n",
        "\n",
        "    def mlp_10_patch(activation, hook):\n",
        "        activation[:, -1, :] = clean_cache[hook.name][:, -1, :]\n",
        "        return activation\n",
        "\n",
        "    patched_logits = model.run_with_hooks(\n",
        "        corrupted_tokens,\n",
        "        fwd_hooks=[\n",
        "            (\"blocks.8.attn.hook_z\", head_8_patch),\n",
        "            (\"blocks.8.hook_mlp_out\", mlp_8_patch),\n",
        "            (\"blocks.10.attn.hook_z\", head_10_patch),\n",
        "            (\"blocks.10.hook_mlp_out\", mlp_10_patch)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return patched_logits\n",
        "\n",
        "# Then in the test loop:\n",
        "for clean_prompt, corrupted_prompt, expected_answer in fact_tests:\n",
        "    print(f\"\\n=== Testing: {clean_prompt} ===\")\n",
        "\n",
        "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=False)\n",
        "    corrupted_tokens = model.to_tokens(corrupted_prompt, prepend_bos=False)\n",
        "\n",
        "    clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
        "    corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
        "\n",
        "    clean_pred = model.to_string(clean_logits[0, -1].argmax())\n",
        "    corrupted_pred = model.to_string(corrupted_logits[0, -1].argmax())\n",
        "    patched_logits = patch_multi_layer_circuit_flexible(clean_cache, corrupted_cache, corrupted_tokens)\n",
        "    patched_pred = model.to_string(patched_logits[0, -1].argmax())\n",
        "\n",
        "    print(f\"Expected: {expected_answer}\")\n",
        "    print(f\"Clean prediction: {clean_pred}\")\n",
        "    print(f\"Corrupted prediction: {corrupted_pred}\")\n",
        "    print(f\"Circuit patched prediction: {patched_pred}\")\n",
        "    print(f\"Circuit helped: {patched_pred == clean_pred}\")"
      ],
      "metadata": {
        "id": "QcHEPYUYYWQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y0nIbhJGYbXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}